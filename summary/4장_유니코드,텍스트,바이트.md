코드 포인트를 바이트로 변환하는 것을 “인코딩”, 바이트를 코드 포인트로 변환하는 것을 “디코딩”이라고 한다. (파이썬2의 경우 약간 Go랑 비슷한 것 같기도 하고?)

```python
>>> s = '한글'
>>> len(s)
2
>>> b = s.encode('utf8')
>>> b
b'\xed\x95\x9c\xea\xb8\x80' 

'''
# \x = 뒤에 16진수 1byte (8bit)을 16진수 두 개로 풀어낸 것
\xed 를 예로 들면:
e (1110) - 4비트
d (1101) - 4비트

ed = 1110 1101 (8비트 = 1바이트)
'''
>>> len(b)
6
```

파이썬 배포본에는 텍스트 바이트로, 혹은 그 반대로 변환하는 100여 개의 코덱(인코더/디코더)이 포함됨.

### UnicodeEncodeError

대부분 비UTF 코덱은 유니코드 문자의 일부만 처리 가능. 텍스트를 바이트로 변환할 때 문자가 대상 인코딩에 정의되어있지 않았고, 인코딩 메서드나 함수의 errors 인수에 별도의 처리기가 지정되지 않았으면 UnicodeEncodeError가 발생.

아마(?) 아스키는 모든 인고딩의 부분집합. 텍스트가 아스키 문자만으로 구성되면 인코딩은 언제나 문제없이 작동함.

### UnicodeDecodeError의 처리

모든 바이트 패텬이 유요한 아스키 문자가 될 수 없음. UTF-8, 16 등도 마찬가지. 따라서 이진 시퀀스 → 텍스트 변환시 해당 인코딩의 정당한 문자 변환 안 되면 UnicodeDecodeError 발생.

### 예상과 달리 인코딩된 모듈 로딩시 발생하는 SyntaxError

GNU/리눅스와 macOS 시스템에서는 UTF-8을 널리 사용, 윈도우는 cp1252를 주로 사용 (gpt왈 나라에 따라 다름 근데 utf-8은 아닌 것 처럼 보임)

### 바이트 시퀀스의 인코딩 방식을 알아내는 방법

→ 간단히 말 하면 알아낼 수 없음. 인코딩 정보는 반드시 명시 되어야 함. HTTP나 XML 같은 통신 프로토콜이나 파일 포맷은 내용이 어떻게 인코딩되었는지 명시하는 헤더가 있음.

일단 바이트 스트림이 자연어라고 간주되면, 자연어에도 규칙과 제한이 있다는 점을 고려해서 경험과 통계로 추정까지는.. 가능 ‘Chardet-범용 문자 인코딩 탐지기’

### BOM

BOM(Byte Order Mark)은 텍스트 파일의 맨 앞에 붙는 특별한 유니코드 문자입니다. 주요 목적과 특징을 설명해드리겠습니다

- claude
    1. 기본 개념
    - 파일이 어떤 유니코드 인코딩을 사용하는지 표시하는 마커입니다
    - 파일의 맨 처음에 위치하는 2~4바이트의 특수 문자열입니다
    1. 주요 BOM 표시
    - UTF-8: EF BB BF
    - UTF-16 LE (Little Endian): FF FE
    - UTF-16 BE (Big Endian): FE FF
    - UTF-32 LE: FF FE 00 00
    - UTF-32 BE: 00 00 FE FF
    1. 실제 사용 사례

    ```python
    # UTF-8 파일에서 BOM 확인하기
    with open('example.txt', 'rb') as f:
        bom = f.read(3)
        if bom == b'\\xef\\xbb\\xbf':
            print("UTF-8 with BOM")
        else:
            print("No BOM or different encoding")
    
    ```

    1. 장단점
       장점:
    - 텍스트 에디터가 파일의 인코딩을 자동으로 인식 가능
    - 엔디언 방식을 명확히 알 수 있음

  단점:

    - 파일 크기가 약간 증가
    - 일부 프로그램에서 BOM을 잘못 처리할 수 있음
    - 특히 UTF-8의 경우 BOM이 불필요한 경우가 많음
    1. 주의사항
    - 웹 개발 시 UTF-8 BOM이 있으면 HTTP 헤더에 문제를 일으킬 수 있음
    - XML 파일은 BOM 없이도 인코딩을 선언할 수 있어 보통 BOM을 사용하지 않음
    - 특히 리눅스/유닉스 시스템에서는 BOM 없는 UTF-8을 선호

  필요한 경우 특정 프로그래밍 언어에서 BOM을 다루는 구체적인 예제를 추가로 설명해드릴 수 있습니다.


### 텍스트 파일 다루기

텍스트 I/O를 처리하는 가장 훌륭한 방법은 ‘유니코드 샌드위치’ == 파일을 읽으려고 열 때 등, 입력할 때 bytes는 가능한 빨리 str로 변환. 비즈니스 논리에서는 오로지 str객체로만 다룸. 다른 처리를 하는 도중에 인코딩이나 디코딩 하면 안 됨. 출력할 때는 가능한 늦게 str을 bytes로 인코딩..

- GPT o1

  프로그래밍을 하면서 텍스트(문자열)를 다뤄야 할 때, 파일 입출력 등에서 발생할 수 있는 인코딩 문제를 최소화하고 일관성을 유지하기 위해 자주 언급되는 개념이 **‘유니코드 샌드위치(Unicode Sandwich)’** 입니다.

  **유니코드 샌드위치(Unicode Sandwich)란?**

  **간단히 말해, 프로그램 내부에서는 언제나 “유니코드(Unicode)” 문자열로만 처리** 하고,

  파일이나 네트워크, 터미널(콘솔)과의 **입출력 경계 지점에서만 인코딩/디코딩을 수행** 하는 방식을 가리킵니다.

  이를 샌드위치에 비유하면,

    1.	**(입력 단계)** 바깥 세계(파일·네트워크·콘솔 등)에서 들어오는 텍스트는 **‘바이트 문자열’을 유니코드 문자열로 디코딩**하여 프로그램 내부로 가져온다.

    2.	**(프로그램 내부 단계)** 프로그램 안에서는 **모든 문자열을 유니코드**로만 취급하고, 각종 텍스트 연산(조작, 비교, 검색 등)을 수행한다.

    3.	**(출력 단계)** 최종적으로 바깥 세계로 내보낼 때, **유니코드 문자열을 다시 ‘바이트(특정 인코딩)’로 인코딩**하여 출력하거나 전송한다.

  이처럼 “입력 → 유니코드 변환 → (내부 처리) → 출력 시 인코딩”의 구조가 샌드위치처럼 양쪽(입출력)에서 이루어진다고 하여 **‘유니코드 샌드위치’**라고 부릅니다.

  **왜 중요한가?**

    1.	**인코딩 문제 감소**

  내부 문자열 처리를 **항상 유니코드**로만 하게 되면, 중간에 어떤 인코딩을 써야 할지 고민하는 횟수가 크게 줄어듭니다.

  이로 인해 한글, 이모지, 기타 국제 문자 등을 처리할 때 발생할 수 있는 인코딩 에러가 현저히 감소합니다.

    2.	**코드 가독성과 유지보수성 향상**

  프로그램 곳곳에서 문자 인코딩·디코딩을 따로 해주는 대신, 딱 **입력 단계**와 **출력 단계**에서만 확실하게 처리하므로, 어디서 인코딩을 다뤄야 하는지 명확합니다.

  즉, 인코딩 로직이 여기저기 흩어져 있지 않고 경계에서 한 번씩만 존재하게 되어 유지보수가 쉬워집니다.

    3.	**표준화된 처리**

  대부분의 현대 언어(특히 파이썬, 자바, C# 등)는 이미 내부 문자열을 유니코드로 처리합니다.

  그 흐름에 맞추어 “유니코드 샌드위치” 전략을 쓰면, 언어가 제공하는 표준 라이브러리를 활용하기 쉬워지고, 국제화(i18n)나 다국어 지원에도 유리합니다.

  **간단한 예시 (파이썬 기준)**

    ```python
    def process_text(file_path):
        # (1) 입력 단계: 파일에서 읽어올 때 디코딩을 수행 (일반적으로 파이썬 open은 UTF-8 디폴트를 권장)
        with open(file_path, mode='r', encoding='utf-8') as f:
            text = f.read()  # 이 시점부터 text는 유니코드 문자열(파이썬의 str 타입)
    
        # (2) 내부 처리 단계: 유니코드 문자열(text)로 필요한 작업 수행
        text = text.upper()  # 예: 대문자로 변환
    
        # (3) 출력 단계: 다른 파일에 저장하거나 네트워크로 보낼 때 인코딩을 수행
        output_file = file_path + ".processed"
        with open(output_file, mode='w', encoding='utf-8') as f:
            f.write(text)  # 이 시점에서 f.write가 다시 바이트로 인코딩해 디스크에 기록
    ```

  이 예시에서는

  •	**파일 열기 시점**(입력 단계)에서 encoding='utf-8'을 지정하여, 파일의 바이트를 **유니코드 문자열**로 디코딩.

  •	내부에서는 text를 순수 유니코드 문자열로만 다룸.

  •	다른 파일에 쓸 때(출력 단계) encoding='utf-8'으로 **유니코드 문자열**을 바이트(UTF-8)로 인코딩하여 저장.

  **정리**

  •	**유니코드 샌드위치** = **입력 시 디코딩 → (내부는 유니코드) → 출력 시 인코딩**

  •	이렇게 하면 인코딩 오류가 대폭 줄어들고, 프로그램 구조가 명확해집니다.

  •	현대 프로그래밍 환경에서는 모든 문자열을 유니코드로 다루는 것이 사실상 표준이므로,

  텍스트 입출력을 다루는 가장 깔끔하고 안전한 방식이라 할 수 있습니다.

  즉, 질문하신 “유니코드 샌드위치”는 **“프로그램 내부는 유니코드로, 바깥(입출력)에서만 인코딩/디코딩을 수행하자”**는 전략을 가리키며,

  텍스트를 다룰 때 흔히 발생하는 복잡한 문제들을 피하는 가장 효과적인 방법 중 하나입니다.


TIP : 여러 컴퓨터나 여러 상황에서 실행되어야 하는 코드는 절대 기본 인코딩에 의존하면 안 됨! 컴퓨터마다 기본 인코딩이 다를 수 있음. 무조건 명시! 윈도우에서는.. 명시안 하면 곤란할 수도!

### 유니코드 정규화 제대로 비교하기

유니코드에는 결합 문자가 있어서 문자열 비교가 간단하지 않음. 앞 문자에 연결되는 발음 구별 기호(diactritical mark)는 인쇄할 때 앞 문자와 하나로 결합되어 출력됨. 아래와 같은 이유 때문에 정규화가 필요함!!

1.	**유니코드**는 전 세계 문자를 표현하는 표준으로,

	“단일 코드 포인트(Precomposed)”와 “기본 문자 + 결합 문자(Decomposed)” 둘 다 허용합니다.
         ex)  “U+AC00 (가)”처럼 ‘이미 합쳐진’ 형태를 쓰거나, “U+1100 (초성 ㄱ) + U+1161 (중성 ㅏ)” 식으로 쪼개서 표현할 수도 있음

2.	**OS나 파일 시스템**은 내부적으로 “문자열을 어떤 정규화 형태(NFC/NFD)로 저장할지” 정책이 다를 수 있습니다.

	특히 macOS HFS+(NFD) vs Windows NTFS(NFC) 문제 등.

3.	결과적으로 **시각적으로 똑같은 글자**가 내부적으로 **서로 다른 코드 시퀀스**가 되어 문자열 비교, 파일 이름 동기화, 검색 등에 혼란을 야기합니다.

4.	이를 해소하기 위해 **정규화(Normalization)** 를 수행해,

	“똑같아 보이는 문자는 실제로도 동일한 코드 시퀀스”로 맞추어 주면 문제가 크게 줄어듭니다.

- GPT - 꼭 읽기. 재미 + 유익

  유니코드에서 **결합 문자(Combining Character)** 문제는 “시각적으로 동일하게 보이는 문자가 내부적으로 서로 다른 코드 포인트 시퀀스”를 가질 수 있기 때문에 발생합니다.

  예컨대, 알파벳 e에 악센트 기호(´)가 결합된 문자는 다음 두 가지 방식으로 표현할 수 있습니다.

    1.	**단일 문자** U+00E9 (“é”)

    2.	**조합형 시퀀스** U+0065(“e”) + U+0301(“´”)

  두 경우 모두 사람이 보기에는 똑같은 “é”로 보이지만, 컴퓨터 입장에서는 내부적으로 **서로 다른 바이트/코드 포인트 열**로 처리됩니다.

  이처럼 **앞 문자 + 결합 문자** 방식과 **미리 조합된 단일 코드 포인트** 방식이 혼재되어 있으면 다음과 같은 문제가 생깁니다.

  **1. 문자열 비교 시 문제**

  •	시각적으로 동일하게 보이는 두 문자열이라도,

  내부 코드 포인트가 다르면 **‘==’ 연산 등 단순 비교에서 다르다고 인식**하게 됩니다.

  예를 들어, “é”가 “U+00E9”로 구성된 문자열과 “U+0065 U+0301”로 구성된 문자열은,

  화면상 똑같아 보이지만 이진(바이트) 비교로는 다른 값입니다.

    ```
    # 파이썬 예시
    s1 = "é"  # 내부적으로 U+00E9
    s2 = "é" # 내부적으로 U+0065 U+0301
    print(s1 == s2)  # False가 나올 수 있음
    ```

  이렇게 “똑같아 보이는 문자”들이 단순 비교에서 다르게 취급되면,

  **검색, 해시, 사전 키 값 비교, 파일 시스템 경로 비교** 등 여러 곳에서 예기치 않은 오류가 발생합니다.

  **2. 텍스트 처리(정렬, 검색, 길이 계산 등) 시 비일관성**

  •	문자열의 길이를 계산할 때도, **결합 문자**가 있는 문자열을 단순히 “코드 포인트 수”로 세면 사람이 생각하는 글자 수와 달라질 수 있습니다.

  •	텍스트를 정렬(sort)하거나 부분 문자열 검색(substring 검색) 등을 할 때도,

  “e + 악센트”를 하나의 논리적 단위로 볼 것인지, 두 개의 문자로 볼 것인지 애매해질 수 있습니다.

  예를 들어,

  •	“멜론”(멜 + 론)이 자모 단위로 나눠져 들어오거나,

  •	“é”가 결합 문자로 들어왔는데 어떤 로직은 결합 문자를 별개의 문자로 인식한다면,

  같은 파일명이나 문자열이라도 OS나 환경에 따라 인식 결과가 달라질 수 있습니다.

  **3. 운영체제나 파일 시스템 간 호환성 문제**

  •	일부 OS나 파일 시스템(특히 예전 macOS 파일 시스템 HFS+)은 **NFD**(Normalization Form Decomposed)로 파일 이름을 저장하고,

  •	다른 OS/파일 시스템(Windows NTFS 등)은 **NFC**(Normalization Form Composed)로 저장하여

  시각적으로 동일한 파일 이름이 서로 다른 코드 포인트로 표현되는 상황이 벌어집니다.

  •	이 때, 두 OS 간 파일 이름을 동기화하거나 복사할 때 “파일이름이 같아 보이는데 중복으로 간주”되거나, “파일이름이 달라서 에러 발생” 등의 혼선이 생길 수 있습니다.

  **왜 정규화(Normalization)가 필요한가?**

  위와 같은 문제를 줄이기 위해 **유니코드 문자열을 미리 ‘표준화된 형태’로 변환해 주는 과정**이 필요합니다. 이를 **‘정규화(Normalization)’**라고 합니다.

  유니코드 표준은 다음과 같은 정규화 형식을 정의합니다.

  •	**NFC** (Normalization Form C)

  “가능한 한 글자를 단일 포인트로 합쳐 쓰는(Composed) 방식”

  •	**NFD** (Normalization Form D)

  “가능한 한 글자를 결합 문자로 쪼개는(Decomposed) 방식”

  •	(그 외 NFKC, NFKD 등이 있지만, 일반 텍스트 처리에서는 주로 NFC/NFD가 많이 쓰임)

  **정규화 절차**를 통해, 시각적으로 동일한 문자열을 모두 하나의 형태(NFC 혹은 NFD 등)로 통일해둔 다음에 비교나 저장을 하면,

  “서로 다른 방식으로 결합된 문자열”로 인해 발생하는 불일치를 방지할 수 있습니다.

  **정리**

  •	결합 문자(Combining Character)는 시각적으로 동일하게 보이지만 내부 포인트 시퀀스가 달라 **단순 문자열 비교**를 어렵게 만듭니다.

  •	파일 시스템이나 OS, 언어 환경마다 문자열을 다루는 방식이 달라 “서로 다른 정규화 형태”가 섞여 들어올 때 **알 수 없는 에러**가 발생할 수 있습니다.

  •	이런 혼선을 줄이고, “동일한 텍스트는 동일하게 처리”하기 위해서는 유니코드 정규화(NFC/NFD 등)를 통해 **문자열을 일관된 형태로 통일**해야 합니다.

  즉, **결합 문자가 유발하는 문제 = ‘동일한 텍스트를 다양한 코드 시퀀스로 표현 가능’ → 검색, 비교, 파일명 처리, 정렬 등에서 오류**이며,

  이를 해결하기 위해 **정규화**를 적용해주면 “한 가지(또는 사전에 정해둔) 형태”로 통일할 수 있어 문제를 크게 줄일 수 있습니다.

  우선 크게 **유니코드(Unicode)와 UTF-8**의 관계를 이해하면 좀 더 명확해집니다.

  **1. 유니코드(Unicode) vs UTF-8**

    1.	**유니코드(Unicode)**

  •	“전 세계 모든 문자를 코드 포인트(Code Point)라는 통일된 번호 체계로 표현하자”라는 **추상적 표준**입니다.

  •	예: A는 U+0041, 한은 U+D55C, é는 U+00E9 등등.

  •	또한 유니코드는 **‘결합 문자(Combining Character)’** 개념을 지원하기 때문에,

  같은 글자를 **미리 조합된 단일 코드 포인트(Precomposed)** 로 쓸 수도 있고,

  기본 글자(예: e) + 결합 악센트(예: ́, U+0301) 형태로 쪼개 쓸 수도 있습니다.

    2.	**UTF-8**

  •	유니코드 ‘코드 포인트’를 실제 컴퓨터에서 사용할 때 **바이트(byte) 열로 변환(인코딩)하는 방식** 중 하나입니다.

  •	“U+00E9”와 같은 코드를 메모리나 파일로 저장할 때는 “C3 A9” (16진수) 같은 바이트 시퀀스로 바꿔줍니다.

  •	만약 “U+0065(U+0301)” 시퀀스를 UTF-8로 인코딩하면 “65 CC 81” (16진수) 처럼 또 다른 바이트 시퀀스가 됩니다.

  •	즉, **유니코드(추상)가 결정되면 그 코드를 UTF-8, UTF-16 등으로 ‘실제 바이트’로 바꾼다**고 생각하면 됩니다.

  **2. 결합 문자 시퀀스도 “공식 유니코드 표현”이다**

  유니코드는 특정 글자를 나타낼 때, **“하나의 코드 포인트”**(예: U+00E9)로도 지원하고,

  **“기본 문자 + 결합 문자”**(예: U+0065 + U+0301) 조합도 지원합니다.

  결합 문자 방식은 “음성, 악센트, 발음 구별 기호” 등을 **기본 문자에 덧붙여 표현**하는 유니코드의 공식 기능입니다.

  예를 들어,

  •	é는 “U+00E9”로 쓸 수도 있고, “U+0065(U+0301)”로 쓸 수도 있으며,

  •	한글도 “U+AC00 (가)”처럼 ‘이미 합쳐진’ 형태를 쓰거나, “U+1100 (초성 ㄱ) + U+1161 (중성 ㅏ)” 식으로 쪼개서 표현할 수도 있습니다.

  **둘 다 표준 유니코드**이므로 “어느 쪽이 틀렸다”가 아니라, **서로 다른 표기 방식**일 뿐입니다.

  **3. 그런데 왜 OS에 따라 달라지나?**

  **(1) 파일 시스템의 정규화 정책**

  •	예를 들어 **macOS의 HFS+ 파일 시스템**은 “파일 이름”을 저장할 때 **NFD(분해형)** 를 강제 적용합니다.

  •	즉, é라는 글자 이름을 저장하면 내부적으로는 “e(U+0065) + (U+0301)” 형태가 됩니다.

  •	반면 **Windows의 NTFS** 등은 주로 **NFC(조합형)** 로 취급하는 편입니다.

  •	같은 é라도 내부적으로는 주로 “U+00E9”로 저장됩니다.

  그래서 macOS에서 만든 파일명을 Windows에서 열면, **똑같이 보이지만 내부 코드(바이트)는 달라**서 충돌이 일어날 수 있습니다.

  (“이름이 같아 보이는데 왜 다르다고 나오지?” 같은 문제)

  **(2) 프로그램/라이브러리마다 처리 방식**

  •	어떤 프로그램(문서 편집기 등)은 결합 문자를 자동으로 합쳐서 보여주기도 하고,

  •	다른 프로그램은 분해형 그대로 처리하여 문자열 비교 시 “다른 문자열”이라고 볼 수도 있습니다.

  이렇듯 유니코드가 **“하나의 표준”**임에도, **표준에서 허용하는 여러 가지 표기 방식**(NFC, NFD 등) 중 **어떤 것을 실제로 쓸 것인지**는

  OS/프로그램/라이브러리마다 다를 수 있습니다.

  **4. 정규화(Normalization)가 필요한 이유**

  •	**정규화**란, 결합 문자로 분해된 문자열(NFD)이나 합쳐진 문자열(NFC) 등 여러 형태 중

  **한 가지 형태**로 **통일**시키는 과정을 말합니다.

  •	이를 통해 **시각적으로 동일한 문자열**을 실제 코드(바이트) 레벨에서도 **동일하게** 만들어줍니다.

  •	만약 이 과정을 생략하면, “겉보기로 똑같은데 내부 인코딩이 달라서 충돌, 검색 실패, 파일 중복” 등이 벌어질 수 있습니다.

  **결론 정리**

    1.	**유니코드**는 전 세계 문자를 표현하는 표준으로,

  •	“단일 코드 포인트(Precomposed)”와 “기본 문자 + 결합 문자(Decomposed)” 둘 다 허용합니다.

    2.	**UTF-8**은 유니코드를 실제 바이트로 변환하는 인코딩 방식일 뿐이므로,

  •	어떤 유니코드 코드 포인트(또는 시퀀스)를 쓰느냐에 따라 결과 바이트가 달라집니다.

    3.	**OS나 파일 시스템**은 내부적으로 “문자열을 어떤 정규화 형태(NFC/NFD)로 저장할지” 정책이 다를 수 있습니다.

  •	특히 macOS HFS+(NFD) vs Windows NTFS(NFC) 문제 등.

    4.	결과적으로 **시각적으로 똑같은 글자**가 내부적으로 **서로 다른 코드 시퀀스**가 되어

  •	문자열 비교, 파일 이름 동기화, 검색 등에 혼란을 야기합니다.

    5.	이를 해소하기 위해 **정규화(Normalization)** 를 수행해,

  •	“똑같아 보이는 문자는 실제로도 동일한 코드 시퀀스”로 맞추어 주면 문제가 크게 줄어듭니다.

  즉, **조합형 시퀀스는 유니코드 표준의 공식 기능이며, UTF-8도 모든 유니코드 코드 포인트를 인코딩할 수 있으니 당연히 지원합니다**.

  다만 **사용 환경(OS, 파일시스템 등)이 어떤 정규화 방식을 쓰느냐**에 따라 **실제 저장·비교 방식이 달라지는 것**이죠.

  그래서 “같은 유니코드 문자인데도 내부 표현(코드)이 다르다”는 상황이 생기고, 그걸 “정규화”로 해결하는 것입니다.


### 유니코드 텍스트 정렬하기

파이썬은 각 시퀀스의 항목들을 하나하나 비교함으로써 어떠한 자료형의 시퀀스도 정렬할 수 있음. 문자열에서는 코드 포인트를 비교함. 그런데 이렇게 비교하면 아스키 이외의 문자를 사용할 때 원치 않는 결과가 나올 수 있음. ex) 브라질의 사례가 나옴. 뭐 어떤 부호가 정렬에 영향을 미치면 안 됨. (사실 잘 모르겠음 ㅎㅎ)

locale을 뭐 잘 사용하자.

- locale에 대한 GPT의 설명

  **locale**(로케일)은 간단히 말해, **사용자(또는 환경)의 언어·지역 설정**에 관련된 규칙과 정보를 담은 개념입니다.

  즉, “이 시스템(또는 프로그램)은 어떤 **언어**(Language), 어떤 **지역**(Region), 어떤 **정렬 규칙**(Collation) 등을 쓰고 있는가?”를 정의하고,

  그 설정에 따라 **날짜·숫자·통화·문자열 처리 방식** 등이 결정됩니다.

  파이썬에서 locale 모듈은 이런 지역화(localization) 기능을 다루기 위해 제공됩니다.

  **1. locale(로케일)이란?**

  •	**언어(Language), 국가/지역(Region), 문자 인코딩(Encoding) 등을 묶어** 특정 문화권에서 사용하는 형식적 규칙을 정해놓은 것.

  예: ko_KR.UTF-8 (한국어, 대한민국, UTF-8 인코딩), en_US.UTF-8 (영어, 미국, UTF-8 인코딩) 등

  •	이 로케일 정보는 **문자열 정렬 순서(알파벳/자모 순), 대소문자 변환 규칙, 날짜·시간·숫자·통화 포맷** 등에 영향을 줍니다.

  예를 들어,

  •	**프랑스어(fr_FR)**에서는 소수점 대신에 쉼표(,)를 사용할 수 있고,

  •	**독일어(de_DE)**에서는 문자열을 정렬할 때 ‘ß(Eszett)’ 처리 방식을 특별히 고려해야 하며,

  •	**터키어(tr_TR)**에서는 영어의 “i”를 대문자로 만들면 “İ(점이 있는 대문자 I)”가 되어야 하는 등

  언어별로 다양한 규칙이 존재합니다.

  **2. 파이썬에서의 locale 모듈**

  파이썬 표준 라이브러리의 locale 모듈은 **현재 환경(또는 임의로 지정한) 로케일 설정**에 접근·변경하거나,

  그 로케일에 맞춘 **문자열 정렬**, **숫자/통화 포맷** 등을 제공하는 기능을 합니다.

    ```
    import locale
    
    # 현재 시스템의 로케일 확인
    current_locale = locale.getlocale()
    print(current_locale)  # 예: ('ko_KR', 'UTF-8')
    
    # 로케일을 특정 언어/국가로 설정
    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')
    
    # 다시 확인
    print(locale.getlocale())  # 예: ('en_US', 'UTF-8')
    
    # 로케일별 숫자 포맷
    locale.setlocale(locale.LC_ALL, 'de_DE.UTF-8')
    print(locale.format_string("%d", 1234567, grouping=True))
    # '1.234.567' 처럼 찍힐 수 있음 (독일식 천단위 구분)
    ```

  •	getlocale(), setlocale() 등을 통해 로케일을 조회하거나 변경할 수 있습니다.

  •	문자열 정렬이나 숫자/통화 포맷 등에서 해당 로케일의 규칙을 자동 적용할 수 있습니다.

  **3. 유니코드(Unicode)와의 관계**

    1.	**로케일이 다루는 영역**

  •	문자열을 **어떤 규칙**으로 정렬할지(예: a < b < c vs. ä < ö < ü, 자모 분리 등)

  •	대소문자 변환(터키어 i → İ, 그리스어 σ vs ς 등)

  •	날짜·시간·숫자 서식(월/일 순서, AM/PM 표기 등)

  •	문자 인코딩 정보(예: UTF-8, ISO-8859-1, EUC-KR 등)

  즉, 유니코드 자체가 “문자를 번호(코드 포인트)로 할당하고, 기본적인 특성을 정의”했다면,

  **locale**은 “문화권별로 **문자를 다루는 규칙(정렬, 대소문자, 서식)을 구체적으로 정하는 것”**이라고 볼 수 있습니다.

    2.	**유니코드 vs. 로케일**

  •	**유니코드**: 전 세계의 문자를 표현하기 위한 하나의 표준(추상적 코드 포인트 체계).

  •	**UTF-8**: 그 유니코드 코드를 실제 바이트로 인코딩하는 방식.

  •	**로케일**: 어느 언어권(ko_KR, en_US, etc)인지, 어떤 인코딩(UTF-8 등)을 쓰는지, 해당 언어권에 맞춘 정렬·표기 규칙은 무엇인지.

  예컨대, **한글(UTF-8)**로 쓰는 환경이라도, 로케일이 ko_KR.UTF-8인지 en_US.UTF-8인지에 따라

  •	날짜 표기(YYYY-MM-DD vs. MM/DD/YYYY vs. 2025년 2월 15일 등)

  •	문자열 정렬 시 한글 자모 순서 준수 여부

  등이 달라질 수 있습니다.

    3.	**정규화(Normalization)와 로케일**

  •	유니코드 정규화(NFC, NFD 등)는 “글자 자체가 분해형인지 조합형인지”를 통일하는 문제.

  •	로케일은 “해당 글자를 어느 언어 규칙에 맞춰 정렬·대소문자 변환·입출력 인코딩을 할 건지” 결정하는 문제.

  •	서로 다른 레이어의 문제이지만, **모두 “다국어·유니코드 처리 시 흔히 부딪히는”** 영역이라 함께 언급되는 경우가 많습니다.

  **4. 왜 필요한가?**

    1.	**다양한 언어·지역 환경에서의 호환성**

  •	국제화(i18n)를 지원하는 애플리케이션이라면, 사용자 환경에 맞게 날짜/시간/통화/문자열 정렬을 해줘야 합니다.

  •	로케일 설정이 없다면, 예를 들어 “숫자 천 단위 구분을 콤마로 할지, 점으로 할지”를 일일이 하드코딩해야 해서 불편하고 확장성이 떨어집니다.

    2.	**문자열 정렬(Collation), 대소문자 변환**

  •	어떤 언어는 알파벳이 A, B, C, … 순서가 아닐 수 있고, 악센트(é, è…) 등 위치에 따라 정렬 순서가 달라질 수 있습니다.

  •	한글도 초성/중성/종성을 어떻게 정렬할지 규칙이 필요합니다.

  •	터키어에서 i → I 변환은 단순 대문자로 만들면 I이 아니라 İ가 되어야 하는 이슈 등, 언어별 예외가 많습니다.

  •	locale을 통해 이러한 언어별 정렬·대소문자 변환 규칙을 적용할 수 있습니다.

    3.	**숫자/통화/날짜 형식**

  •	지역마다 숫자 소수점·천 단위 구분, 화폐 기호 표기, 날짜 표기 순서, 요일/월 이름 등이 다릅니다.

  •	locale 설정을 통해 자동으로 적절한 포맷을 선택·적용할 수 있습니다.

  **정리**

  •	**유니코드**: 전 세계 문자에 대한 “코드 포인트” 표준 (어떤 문자가 몇 번 코드인지).

  •	**UTF-8**: 유니코드 코드를 바이트로 변환하는 인코딩 방식.

  •	**로케일(locale)**: “언어·지역·인코딩 등을 바탕으로 **문자열 정렬, 대소문 변환, 날짜/숫자/통화 표기**” 등의 규칙을 정의한 환경 설정.

  결국 “유니코드와 로케일”은 **텍스트 국제화(i18n)·지역화(L10n)에서 중요한 두 축**이라 할 수 있습니다.

  •	유니코드는 **“어떤 문자를 쓰느냐”(표현 범위)** 를 해결,

  •	로케일은 **“그 문자와 데이터를 문화권 규칙에 따라 어떻게 보여주고 처리하느냐”(정렬, 포맷, 입출력)** 를 담당합니다.

  파이썬의 locale 모듈은 이 로케일 정보를 조회/설정하여 파이썬 프로그램에서 지역화된 형식(숫자/날짜/문자열 정렬 등)을

  쉽게 적용할 수 있도록 해주는 **표준 라이브러리**입니다.


### 유니코드 데이터베이스

유니코드 표준은 수많은 구조화된 텍스트 파일의 형태로 하나의 완전한 데이터베이스를 제공함.

- 파이썬 유니코드 데이터베이스에 대한 GPT 설명

  파이썬에는 **유니코드(Unicode) 관련 정보**를 담고 있고, 각종 **문자 특성**(이름·범주·숫자값 등)을 조회할 수 있는 **내장 데이터베이스**가 있습니다.

  바로 **unicodedata 모듈**이 그것입니다.

  **1. unicodedata 모듈이란?**

  •	파이썬 표준 라이브러리에서 제공하는 **유니코드 전용 정보/함수** 모음.

  •	**유니코드 표준**(버전 별)에 의해 정의된

  •	**문자 이름**

  •	**카테고리**(Letter, Mark, Number, Punctuation 등)

  •	**숫자값**(예: Ⅳ → 4)

  •	**정규화(Normalization) 함수**

  등을 이용할 수 있습니다.

  파이썬은 버전별로 **특정 유니코드 버전**을 내장하여, 해당 버전까지 정의된 문자와 특성을 unicodedata가 알고 있습니다.

  예를 들어,

  •	Python 3.10은 [Unicode 13.0](https://docs.python.org/3/library/unicodedata.html)을,

  •	Python 3.11은 [Unicode 14.0](https://docs.python.org/3.11/library/unicodedata.html)

  …

  등을 지원하는 식으로, 파이썬이 업데이트되면 내장된 유니코드 데이터베이스도 최신 버전에 맞춰 갱신됩니다.

  참고: 정확한 “Python 버전 ↔ Unicode 버전” 매핑은 파이썬 공식 문서에서 확인할 수 있습니다.

  **2. 주요 기능**

  **2.1 문자 이름 조회 / 역 lookup**

    1.	**unicodedata.name(char)**

  •	해당 문자(코드 포인트)의 **공식 유니코드 이름**을 반환합니다.

  •	예:

    ```
    import unicodedata
    
    print(unicodedata.name('한'))
    # 'HANGUL SYLLABLE HAN'
    print(unicodedata.name('😀'))
    # 'GRINNING FACE'
    ```

    2.	**unicodedata.lookup(name)**

  •	유니코드의 공식 이름을 가지고 **문자**를 역으로 찾습니다.

  •	예:

    ```
    print(unicodedata.lookup('HANGUL SYLLABLE HAN'))  # '한'
    print(unicodedata.lookup('GRINNING FACE'))        # '😀'
    ```

  **2.2 문자 범주(category), 숫자값(numeric), 결합 등 특성**

    1.	**unicodedata.category(char)**

  •	해당 문자의 **범주**(Letter, Digit, Mark, Punctuation 등)를 알려줍니다.

  •	예:

    ```
    print(unicodedata.category('A'))   # 'Lu' (대문자, Uppercase Letter)
    print(unicodedata.category('가')) # 'Lo' (기타 문자, Other Letter)
    print(unicodedata.category('1'))   # 'Nd' (숫자, Decimal Number)
    print(unicodedata.category(' '))   # 'Zs' (공백, Space Separator)
    ```

    2.	**unicodedata.normalize(form, text)**

  •	“유니코드 정규화(NFC, NFD, NFKC, NFKD)”를 적용하여, **문자열을 특정 정규화 형식**으로 변환해 줍니다.

  •	예:

    ```
    from unicodedata import normalize
    
    s1 = "é"        # U+00E9
    s2 = "e\u0301"  # U+0065 + U+0301
    print(s1 == s2)                       # False (바이트 레벨에서 다름)
    print(normalize('NFC', s1) == normalize('NFC', s2))  # True
    ```

    3.	**unicodedata.combining(char)**

  •	해당 문자가 결합 문자(Combining Character)에 속하는지 여부(0/1)를 반환.

  •	예:

    ```
    # U+0301 (COMBINING ACUTE ACCENT)
    print(unicodedata.combining('\u0301'))  # 230 (0이 아니면 결합 문자)
    # 'é' 자체 (U+00E9)는 결합 문자가 아님 -> 0
    print(unicodedata.combining('é'))       # 0
    ```

    4.	**unicodedata.numeric(char, default=None)**

  •	해당 문자가 나타내는 **숫자값**을 반환합니다. (예: 로마 숫자, 분수 기호 등)

  •	예:

    ```
    print(unicodedata.numeric('Ⅳ'))  # 4.0
    print(unicodedata.numeric('⅔'))  # 0.6666666667
    # '한'은 숫자 아님 -> ValueError 발생,
    # default 인자를 지정하면 해당 값 반환
    ```

  **2.3 기타 속성들**

  •	unicodedata.bidirectional(char): 이 문자의 양방향(BiDi) 클래스 (아랍어, 히브리어 등)

  •	unicodedata.decomposition(char): 해당 문자가 분해(NFD)될 때 어떤 시퀀스로 갈리는지

  •	… 등등, 유니코드 표준에 정의된 여러 속성을 조회할 수 있습니다.

  **3. 왜 중요한가?**

    1.	**다국어 처리**

  •	전 세계 모든 문자를 파이썬에서 다룰 때, “이 문자가 무엇인지”, “어떤 카테고리에 속하는지”, “정규화할 때 어떻게 처리해야 하는지”와 같은 **메타데이터**가 필요합니다.

  •	unicodedata가 이 정보를 제공해 주므로, 국제화(i18n)나 텍스트 분석에 매우 유용합니다.

    2.	**정규화(Normalization)**

  •	결합 문자를 다루거나, 서로 다른 OS/filesystem(NFC vs NFD)에서 온 텍스트를 일관성 있게 처리하려면, **정규화**가 필수입니다.

  •	파이썬 내장 unicodedata.normalize() 함수로 손쉽게 처리할 수 있습니다.

    3.	**문자 분류·검색**

  •	“이 문자는 대문자인지 소문자인지, 숫자인지, 기호인지, 결합 문자인지” 등을 파악해야 하는 로직(문자열 필터링, 정렬, 토큰화 등)에 활용할 수 있습니다.

    4.	**유니코드 버전별 호환성**

  •	파이썬이 업그레이드될 때마다 내장된 유니코드 데이터베이스도 새로운 버전을 지원합니다.

  •	예전 파이썬에서는 아직 정의되지 않은 이모지(Emoji)나 새로 추가된 문자들이 최신 파이썬에서는 인식되는 경우도 있습니다.

  **4. 요약**

  •	**파이썬의 유니코드 데이터베이스**는 unicodedata 모듈에 구현되어 있으며,

  유니코드 표준에 따라 각 문자의 이름·카테고리·결합 정보·숫자값·양방향성 등 **다양한 속성**을 조회하고,

  **정규화(NFC, NFD 등) 연산**을 수행할 수 있게 해줍니다.

  •	다국어 텍스트 처리, 정규화, 문자 분류, 이모지 처리 등에서 꼭 필요한 기능이며,

  파이썬 버전이 올라가면(3.x가 바뀌면) 대응하는 **유니코드 버전**도 업데이트되어 **최신 문자**들이 인식·처리됩니다.

  간단히 말해, **“유니코드 문자를 제대로 이해하고 다루기 위한 데이터와 도구를 파이썬 내부에 갖춰 둔 것”**이라고 보시면 됩니다.


### 이중 모드 str 및 bytes API

파이썬 표준 라이브러리에는 str과 bytes 형 인수를 모두 받으며, 자료형에 따라 다르게 작동하는 함수가 있음. re와 os 모듈에 몇 개의 예가 있음.

ex) bytes 형으로 정규 표현식을 만들면 \d \w와 같은 패턴들은 아스키 문자만 매칭되지만, str형으로 이 패턴을 만들면 아스키 문자 외에 유니코드 숫자나 문자도 매칭됨.

GNU/리눅스 커널은 유니코드를 모름 (**리눅스 커널**은 파일명을 *“아무 바이트나 가능한 문자열”*로 간주하며, *유니코드(UTF-8) 규칙을 enforce하지 않는다*는 뜻) 그래서 **사용자 공간(쉘, 파일 탐색기, 애플리케이션)**에서는 “기본이 UTF-8이니까 당연히 UTF-8 문자열일 것”이라 생각하지만, 실제로는 그렇지 않은 바이트열도 파일명으로 존재할 수 있음.

위와같은 문제를 피하고자 파일명이나 경로명을 받는 모든 os 모듈 함수는 str이나 bytes 형의 인수를 받음. 이런 함수를 str인수로 호출하면 인수는 sys.getfilesystemencoding() 함수로 지정된 코덱을 이용해 자동 변환, 운영 체제의 응답도 동일 코뎅을 이용해 디코딩됨.

그러나 이렇게 처리할 수 없는 파일명을 다루거나 수정해야 할 때는 bytes 인수를 os함수에 전달해서 반환된 bytes 값을 가져올 수 있음.

- bytes 인수로 조작해야하는 상황의 예시 by GPT

  일반적으로 리눅스·유닉스 환경에서, **파일명이 OS/파일시스템 차원에서 “그냥 바이트 열(byte sequence)”로 존재**할 수 있기 때문에,

  (앞서 언급했듯 커널이 유니코드를 강제하지 않으므로)

  **시스템이 UTF-8 로케일**이라도 “**유효하지 않은 UTF-8 시퀀스**”가 파일명에 들어갈 수 있습니다.

  이럴 때, 파이썬에서 os 모듈 함수(예: os.listdir, os.unlink, os.rename, …) 등을 **문자열(str)** 인수로 호출하면,

  내부적으로 sys.getfilesystemencoding()(대부분 UTF-8)으로 **인코딩/디코딩**을 시도합니다.

  그런데 **파일명에 깨어진 바이트**가 있으면 디코딩 실패가 발생해 **UnicodeDecodeError**가 터질 수 있죠.

  이 경우, **“해당 파일명을 어떻게든 처리 또는 수정해야 하는데, 문자열 변환이 안 된다면 어떻게?”**

  → 바로 **‘bytes 인수로 OS 함수를 호출’**하는 방식을 써야 합니다.

  파이썬이 내부적으로 인코딩/디코딩을 하지 않고 **“원시 바이트 그대로”**를 시스템 호출에 전달하기 때문에,

  “파일명이 깨진 UTF-8”이라도 리눅스 커널(바이트 레벨에서만 취급) 차원에서는 정상적으로 접근·조작할 수 있습니다.

  **예시 시나리오**

  다음은 **리눅스(또는 유닉스 계열)** 환경에서 가상의 예시입니다.

  (실제로 동작하려면 로컬 환경에 맞춰 테스트해야 합니다.)

    1.	**의도적으로 ‘잘못된 UTF-8 시퀀스’를 포함하는 파일명 생성**

    ```
    # echo 명령어 + 인라인 바이너리(latin-1 etc.)로 만들어서 파일 생성 (유닉스에서 가능한 기법 중 하나)
    # 예: 0xC0 0x80 (UTF-8에서 유효하지 않은 시퀀스)
    printf "\xC0\x80" > bad_filename
    ```

  이렇게 하면, 리눅스 파일 시스템에는 실제로 \xC0\x80 바이트를 이름으로 갖는 파일이 생깁니다.

  (이름 길이 자체가 2바이트, 사람이 보기엔 제어문자처럼 보이거나 아무것도 안 보일 수 있음)

    2.	**파이썬에서 이 디렉토리를 조회**

    ```python
    import os
    import sys
    
    path = '.'  # 예: 현재 디렉토리
    encoding = sys.getfilesystemencoding()
    print("Filesystem encoding:", encoding)  # 대개 'utf-8'
    
    try:
        file_list = os.listdir(path)
        print(file_list)
    except UnicodeDecodeError as e:
        print("UnicodeDecodeError 발생:", e)
    ```

  •	만약 os.listdir가 디렉토리 내 파일명을 전부 **str**로 디코딩하여 반환하려고 시도하면,

  여기서 '\xC0\x80' 같은 잘못된 시퀀스를 만나 **UnicodeDecodeError**가 날 수 있습니다.

    3.	**os.listdir에 bytes를 사용하는 방식**

  파이썬 문서에도 언급되어 있듯,

  os.listdir(path)가 **문자열(str) 리턴**을 하는 버전과, os.listdir(path.encode(...)) 등으로 **바이트(bytes)** 경로를 넘겨

  **파일명도 바이트(bytes) 목록**으로 받아오는 방식을 구분할 수 있습니다.

    ```python
    # bytes 타입 경로로 호출
    path_bytes = b'.'  # 현재 디렉토리를 bytes로
    try:
        file_list_bytes = os.listdir(path_bytes)
        print(file_list_bytes)
        # 결과 예: [b'\xc0\x80'] 처럼 "깨진 UTF-8" 파일명도 그대로 표시됨
    except Exception as e:
        print("Error:", e)
    
    # 이제 bad_filename을 삭제하거나 이름 변경 등 처리도 bytes 인수로 하면 가능
    bad_name = b'\xc0\x80'
    os.rename(bad_name, b'fixed_name')  # 예: raw 바이트 -> 'fixed_name' (정상 UTF-8)
    ```

  •	이렇게 하면 **파이썬이 디코딩 없이** 파일명을 **바이트**로 받아오고, 그대로 OS 함수(rename, unlink, 등)에 바이트로 전달할 수 있습니다.

  •	결과적으로 “깨진 파일명”도 **삭제·이름변경·이동** 등이 가능해지는 것이죠.

  **실제 상황 예시**

  •	**FTP/SFTP 서버**에 윈도우나 다른 시스템에서 업로드된 파일들이 있는데, 파일명이 UTF-8이 아닌 EUC-KR, Shift_JIS, 혹은 그냥 깨진 바이트로 되어 있을 수도 있음.

  •	**리눅스 서버** 쪽에서 이를 처리하려고 할 때, str(UTF-8 기반)로 불러오면 디코딩 에러가 납니다.

  •	그렇지만 서버 관리 스크립트 상에서 “파일 정리/삭제/정규화된 이름으로 리네임” 등을 자동화해야 할 때는, **바이트 모드**로 접근해야만 가능.

  예를 들어,

  •	한 해외 개발자가 ISO-8859-1(라틴1) 환경에서 생성한 파일명을,

  •	UTF-8 로케일 리눅스 서버에서 str로 디코딩하려고 하면 깨져서 오류.

  •	그러나 os.listdir(b'/path') → 결과를 바이트로 받아, 바이트 기준으로 처리 후(예: os.rename) → UTF-8로 재정규화한 이름으로 변경.

  •	이후엔 문제없이 str로도 접근 가능.

  **요점**

    1.	리눅스/유닉스 커널은 파일명을 **임의의 바이트**로 취급하므로, *어떤 인코딩으로도 해석 불가능*한 이름이 존재할 수 있음.

    2.	파이썬에서는 기본적으로 **str 인수/리턴** 시 sys.getfilesystemencoding()으로 인코딩/디코딩을 시도하기 때문에,

  **이상한 바이트**가 섞여 있으면 **UnicodeDecodeError**가 발생해 접근이 불가능.

    3.	이때 **bytes 인수**를 사용하는 os 함수나 os.listdir은 “그 바이트 그대로”를 시스템 콜에 전달 및 반환하므로,

  **깨진 이름**도 잡아서 처리(삭제·변경 등)할 수 있다.

  그것이 곧 “깨진 파일명을 다루거나 수정해야 할 때는 **bytes 인수를 os 함수에 전달**해서 반환된 bytes 값을 가져와 처리한다”는 이유입니다.


## 요약

1문자 != 1바이트